# Loglikelihood of Multinomial


$$
\ln L=\sum_{i=1}^{n}\sum_{j=0}^{J}d_{ij}\ln\left(\frac{\exp\left(\beta_{j}^{\prime}x_{i}\right)}{\sum_{k=0}^{J}\exp\left(\beta_{k}^{\prime}x_{i}\right)}\right)        
$$
 
         

# Score (Gradient) - Logistic Regression
$$
\frac{\partial\ln L_{i}}{\partial\beta}=\left(y_{i}-\Lambda_{i}\right)x_{i}
$$


# Hessian - Logistic Regression



$$
\frac{\partial^{2}\ln L}{\partial\beta\partial\beta^{\prime}}=-\sum_{i}\Lambda_{i}\left(1-\Lambda_{i}\right)x_{i}x_{i}^{\prime}
$$
# Loglikelihood of Multinomial
$$
\ln L=\sum_{i=1}^{n}\sum_{j=0}^{J}d_{ij}\ln
\left(\frac{\exp\left(\beta_{j}^{\prime}x_{i}\right)}
{\sum_{k=0}^{J}
\exp\left(\beta_{k}^{\prime}x_{i}\right)}\right)
$$


# Score (gradient) of  multinomial 

Gradient Steps: 

```python
def score(self, params):
        """
        Score matrix for multinomial logit model log-likelihood

        Parameters
        ----------
        params : array
            The parameters of the multinomial logit model.

        Returns
        -------
        score : ndarray, (K * (J-1),)
            The 2-d score vector, i.e. the first derivative of the
            loglikelihood function, of the multinomial logit model evaluated at
            `params`.

        Notes
        -----

        for :math:`j=1,...,J`

        In the multinomial model the score matrix is K x J-1 but is returned
        as a flattened array to work with the solvers.
        """
        # 6 Variables (K), 7 class problem (J)
        # for gradient do:
        # K x J - 1 matrix (I guess 36 coefficeints 6x(7-1))
        # wendog is one-hot encoding algorithm of class (7 dummies) each class has coefficents
        params = params.reshape(self.K, -1, order='F')
        firstterm = self.wendog[:,1:] - self.cdf(np.dot(self.exog, params))[:,1:] # exog are just variables (944,6) * 6x6    matrix. now substract the results from y true (d below)
         #first term are basically the errors
        return np.dot(firstterm.T, self.exog).flatten()
    
def cdf(self, X):
#          Multinomial logit cumulative distribution function.
    
        eXB = np.column_stack((np.ones(len(X)), np.exp(X))) # add one to the columns now you get 977,7 becuase of 1 (Why?)
        return eXB/eXB.sum(1)[:,None] # divide eXB by sum by column all eXB

            eXB.sum(1)[:,None]
            array([[7.],
                   [7.],
                   [7.],
                   [7.],
                   [7.],
                   [7.],
                   [7.],
                   [7.],
                   [7.],

    
```

$$
\frac{\partial\ln L}{\partial\beta_{j}}=\sum_{i}\left(d_{ij}-\frac{\exp\left(\beta_{j}^{\prime}x_{i}\right)}{\sum_{k=0}^{J}\exp\left(\beta_{k}^{\prime}x_{i}\right)}\right)x_{i}
$$


# Hessian of Multinomial
$$
\frac{\partial^{2}\ln L}{\partial\beta_{j}\partial\beta_{l}}=-\sum_{i=1}^{n}\frac{\exp\left(\beta_{j}^{\prime}x_{i}\right)}{\sum_{k=0}^{J}\exp\left(\beta_{k}^{\prime}x_{i}\right)}\left[\boldsymbol{1}\left(j=l\right)-\frac{\exp\left(\beta_{l}^{\prime}x_{i}\right)}{\sum_{k=0}^{J}\exp\left(\beta_{k}^{\prime}x_{i}\right)}\right]x_{i}x_{l}^{\prime}
$$
